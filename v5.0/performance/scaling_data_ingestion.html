

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scaling Out Data Ingestion &mdash; Citus 5.0.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="Citus 5.0.0 documentation" href="../index.html"/>
        <link rel="next" title="Query Performance Tuning" href="performance_tuning.html"/>
        <link rel="prev" title="Citus Query Processing" href="query_processing.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">
  <div class="header">
  <div class="section-wrap"> 
              <div class="utility-nav">
              </div>
                      <a href="https://www.citusdata.com" title="Citus Data" rel="home" class="logo">
            <img src="https://www.citusdata.com/sites/default/files/logo_0_0.png" alt="Citus Data">
          </a>
              <div class="clear"></div>
  </div>
  </div>
  <div class="nav-wrap menu">
    <div class="section-wrap">
      <ul>
        <li><a href="https://www.citusdata.com/product/citus">Product</a></li>
        <li><a href="https://www.citusdata.com/solutions/applications">Solutions</a></li>
        <li><a href="https://www.citusdata.com/blog">Blog</a></li>
        <li><a href="#">Documentation</a></li>
      </ul>
    </div>
  </div>

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Citus
          

          
          </a>

          
            
            
              <div class="version">
                5.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">About Citus</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../aboutcitus/what_is_citus.html">What is Citus?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aboutcitus/introduction_to_citus.html">Architecture</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-cluster.html">Start Demo Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-real-time.html">Real Time Aggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-user-data.html">Updatable User Data</a></li>
</ul>
<p class="caption"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/requirements.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/development.html">Single-Machine Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/production.html">Multi-Machine Cluster</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Tables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/working_with_distributed_tables.html">Working with distributed tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/append_distribution.html">Append Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/hash_distribution.html">Hash Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/range_distribution.html">Range Distribution (Manual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/querying.html">Querying Distributed Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/postgresql_extensions.html">PostgreSQL extensions</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="query_processing.html">Citus Query Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Scaling Out Data Ingestion</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hash-distributed-tables">Hash Distributed Tables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#real-time-inserts-0-50k-s">Real-time Inserts (0-50k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#real-time-updates-0-50k-s">Real-time Updates (0-50k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#masterless-citus-50k-s-500k-s">Masterless Citus (50k/s-500k/s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#append-distributed-tables">Append Distributed Tables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#master-node-bulk-ingestion-50k-s-100k-s">Master Node Bulk Ingestion (50k/s-100k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#worker-node-bulk-ingestion-100k-s-1m-s">Worker Node Bulk Ingestion (100k/s-1M/s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pre-processing-data-in-citus">Pre-processing Data in Citus</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Query Performance Tuning</a></li>
</ul>
<p class="caption"><span class="caption-text">Administration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/cluster_management.html">Cluster Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/upgrading_citus.html">Upgrading to Citus 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/transitioning_from_postgresql_to_citus.html">Transitioning From PostgreSQL to Citus</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/citus_sql_reference.html">Citus SQL Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/user_defined_functions.html">User Defined Functions Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/metadata_tables.html">Metadata Tables Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/configuration.html">Configuration Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <a href="../index.html">Citus</a>
      <ul>
        <li><a href="/product/citus">Product</a></li>
        <li><a href="/solutions/applications">Solutions</a></li>
        <li><a href="/blog">Blog</a></li>
      </ul>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li>Scaling Out Data Ingestion</li>
    <li class="wy-breadcrumbs-aside">
      
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scaling-out-data-ingestion">
<span id="scaling-data-ingestion"></span><h1>Scaling Out Data Ingestion<a class="headerlink" href="#scaling-out-data-ingestion" title="Permalink to this headline">¶</a></h1>
<p>Citus lets you scale out data ingestion to very high rates, but there are several trade-offs to consider in terms of the throughput, durability, consistency and latency. In this section, we discuss several approaches to data ingestion and give examples of how to use them.</p>
<p>The best method to distribute tables and ingest your data depends on your use case requirements. Citus supports two distribution methods: append and hash; and the data ingestion methods differ between them. You can visit the <a class="reference internal" href="../dist_tables/working_with_distributed_tables.html#working-with-distributed-tables"><span class="std std-ref">Working with distributed tables</span></a> section to learn about the tradeoffs associated with each distribution method.</p>
<div class="section" id="hash-distributed-tables">
<h2>Hash Distributed Tables<a class="headerlink" href="#hash-distributed-tables" title="Permalink to this headline">¶</a></h2>
<p>Hash distributed tables support ingestion using standard single row INSERT and UPDATE commands. This sub-section describes how you maximize insert throughput for hash distributed tables.</p>
<div class="section" id="real-time-inserts-0-50k-s">
<h3>Real-time Inserts (0-50k/s)<a class="headerlink" href="#real-time-inserts-0-50k-s" title="Permalink to this headline">¶</a></h3>
<p>On the Citus master, you can perform INSERT commands directly on hash distributed tables. The advantage of using INSERT is that the new data is immediately visible to SELECT queries, and durably stored on multiple replicas.</p>
<p>When processing an INSERT, Citus first finds the right shard placements based on the value in the distribution column, then it connects to the workers storing the shard placements, and finally performs an INSERT on each of them. From the perspective of the user, the INSERT takes several milliseconds to process because of the round-trips to the workers, but the master can process other INSERTs in other sessions while waiting for a response. The master also keeps connections to the workers open within the same session, which means subsequent queries will see lower response times.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Set</span> <span class="n">up</span> <span class="n">a</span> <span class="n">distributed</span> <span class="n">table</span> <span class="n">containing</span> <span class="n">counters</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">counters</span> <span class="p">(</span><span class="n">c_key</span> <span class="n">text</span><span class="p">,</span> <span class="n">c_date</span> <span class="n">date</span><span class="p">,</span> <span class="n">c_value</span> <span class="nb">int</span><span class="p">,</span> <span class="n">primary</span> <span class="n">key</span> <span class="p">(</span><span class="n">c_key</span><span class="p">,</span> <span class="n">c_date</span><span class="p">));</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;counters&#39;</span><span class="p">,</span> <span class="s1">&#39;c_key&#39;</span><span class="p">,</span> <span class="s1">&#39;hash&#39;</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_worker_shards</span><span class="p">(</span><span class="s1">&#39;counters&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>

<span class="o">--</span> <span class="n">Enable</span> <span class="n">timing</span> <span class="n">to</span> <span class="n">see</span> <span class="n">reponse</span> <span class="n">times</span>
\<span class="n">timing</span>

<span class="o">--</span> <span class="n">First</span> <span class="n">INSERT</span> <span class="n">requires</span> <span class="n">connection</span> <span class="nb">set</span><span class="o">-</span><span class="n">up</span><span class="p">,</span> <span class="n">second</span> <span class="n">will</span> <span class="n">be</span> <span class="n">faster</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-04&#39;</span><span class="p">,</span> <span class="mi">12</span><span class="p">);</span> <span class="o">--</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">10.314</span> <span class="n">ms</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-05&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span> <span class="o">--</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">3.132</span> <span class="n">ms</span>
</pre></div>
</div>
<p>INSERT is currently the only way of adding data to hash-distributed tables. To load data from a file into a hash-distributed table, Citus comes with a command-line tool called copy_to_distributed_table, which mimicks the behaviour of COPY by performing an INSERT for each row in an input (CSV) file. However, the script only uses a single connection, meaning every INSERT waits for several round-trips and throughput is very low by default. However, you can parallelize the script by splitting the input and doing the INSERTs in parallel using xargs, which gives vastly better throughput.</p>
<p>For example, for Linux systems you can use the split command to split the input file into 64 pieces.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="n">chunks</span>
<span class="n">split</span> <span class="o">-</span><span class="n">n</span> <span class="n">l</span><span class="o">/</span><span class="mi">64</span> <span class="n">github_events</span><span class="o">-</span><span class="mi">2015</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mf">0.</span><span class="n">csv</span> <span class="n">chunks</span><span class="o">/</span>
</pre></div>
</div>
<p>Then, you can load each of the chunks in parallel using xargs.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">PGDATABASE</span><span class="o">=</span><span class="n">postgres</span>
<span class="n">find</span> <span class="n">chunks</span><span class="o">/</span> <span class="o">-</span><span class="nb">type</span> <span class="n">f</span> <span class="o">|</span> <span class="n">xargs</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1</span> <span class="o">-</span><span class="n">P</span> <span class="mi">64</span> <span class="n">sh</span> <span class="o">-</span><span class="n">c</span> <span class="s1">&#39;echo $0 `copy_to_distributed_table -C $0 github_events`&#39;</span>
</pre></div>
</div>
<p>To learn more about the copy_to_distributed_table script, you can visit the <a class="reference internal" href="../dist_tables/hash_distribution.html#hash-distribution"><span class="std std-ref">Hash Distribution</span></a> section of our documentation.</p>
<p>To reach high throughput rates, applications should send INSERTs over a many separate connections and keep connections open to avoid the initial overhead of connection set-up.</p>
</div>
<div class="section" id="real-time-updates-0-50k-s">
<h3>Real-time Updates (0-50k/s)<a class="headerlink" href="#real-time-updates-0-50k-s" title="Permalink to this headline">¶</a></h3>
<p>On the Citus master, you can also perform UPDATE, DELETE, and INSERT ... ON CONFLICT (UPSERT) commands on distributed tables. By default, these queries take an exclusive lock on the shard, which prevents concurrent modifications to guarantee that the commands are applied in the same order on all shard placements.</p>
<p>Given that every command requires several round-trips to the workers, and no two commands can run on the same shard at the same time, update throughput is very low by default. However, if you know that the order of the queries doesn&#8217;t matter (they are commutative), then you can turn on citus.all_modifications_commutative, in which case multiple commands can update the same shard concurrently.</p>
<p>For example, if your distributed table contains counters and all your DML queries are UPSERTs that add to the counters, then you can safely turn on citus.all_modifications_commutative since addition is commutative:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">SET</span> <span class="n">citus</span><span class="o">.</span><span class="n">all_modifications_commutative</span> <span class="n">TO</span> <span class="n">on</span><span class="p">;</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-04&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ON</span> <span class="n">CONFLICT</span> <span class="p">(</span><span class="n">c_key</span><span class="p">,</span> <span class="n">c_date</span><span class="p">)</span> <span class="n">DO</span> <span class="n">UPDATE</span> <span class="n">SET</span> <span class="n">c_value</span> <span class="o">=</span> <span class="n">counters</span><span class="o">.</span><span class="n">c_value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</pre></div>
</div>
<p>Note that this query also takes an exclusive lock on the row in PostgreSQL, which may also limit the throughput. When storing counters, consider that using INSERT and summing values in a SELECT does not require exclusive locks.</p>
<p>When the replication factor is 1, it is always safe to enable citus.all_modifications_commutative. Citus does not do this automatically yet.</p>
</div>
<div class="section" id="masterless-citus-50k-s-500k-s">
<h3>Masterless Citus (50k/s-500k/s)<a class="headerlink" href="#masterless-citus-50k-s-500k-s" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This section is currently experimental and not a guide to setup masterless clusters in production. We are working on providing official support for masterless clusters including replication and automated fail-over solutions. Please contact us at <a class="reference external" href="mailto:engage&#37;&#52;&#48;citusdata&#46;com">engage<span>&#64;</span>citusdata<span>&#46;</span>com</a> if your use case requires multiple masters.</p>
</div>
<p>It is technically possible to create the distributed table on every node in the cluster. The big advantage is that all  queries on distributed tables can be performed at a very high rate by spreading the queries across the workers. In this case, the replication factor should always be 1 to ensure consistency, which causes data to become unavailable when a node goes down. All nodes should have a hot standby and automated fail-over to ensure high availability.</p>
<p>To allow DML commands on the distribute table from any node, first create a distributed table on both the master and the workers:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">data</span> <span class="p">(</span><span class="n">key</span> <span class="n">text</span><span class="p">,</span> <span class="n">value</span> <span class="n">text</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;key&#39;</span><span class="p">,</span><span class="s1">&#39;hash&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>Then on the master, create shards for the distributed table with a replication factor of 1.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">/*</span> <span class="n">Create</span> <span class="mi">128</span> <span class="n">shards</span> <span class="k">with</span> <span class="n">a</span> <span class="n">single</span> <span class="n">replica</span> <span class="n">on</span> <span class="n">the</span> <span class="n">workers</span> <span class="o">*/</span>
<span class="n">SELECT</span> <span class="n">master_create_worker_shards</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
<p>Finally, you need to copy and convert the shard metadata from the master to the workers. The logicalrelid column in pg_dist_shard may differ per node. If you have the dblink extension installed, then you can run the following commands on the workers to get the metadata from master-node.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">pg_dist_shard</span> <span class="n">SELECT</span> <span class="o">*</span> <span class="n">FROM</span>
<span class="n">dblink</span><span class="p">(</span><span class="s1">&#39;host=master-node port=5432&#39;</span><span class="p">,</span>
       <span class="s1">&#39;SELECT logicalrelid::regclass,shardid,shardstorage,shardalias,shardminvalue,shardmaxvalue FROM pg_dist_shard&#39;</span><span class="p">)</span>
<span class="n">AS</span> <span class="p">(</span><span class="n">logicalrelid</span> <span class="n">regclass</span><span class="p">,</span> <span class="n">shardid</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">shardstorage</span> <span class="n">char</span><span class="p">,</span> <span class="n">shardalias</span> <span class="n">text</span><span class="p">,</span> <span class="n">shardminvalue</span> <span class="n">text</span><span class="p">,</span> <span class="n">shardmaxvalue</span> <span class="n">text</span><span class="p">);</span>

<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">pg_dist_shard_placement</span> <span class="n">SELECT</span> <span class="o">*</span> <span class="n">FROM</span>
<span class="n">dblink</span><span class="p">(</span><span class="s1">&#39;host=master-node port=5432&#39;</span><span class="p">,</span>
       <span class="s1">&#39;SELECT * FROM pg_dist_shard_placement&#39;</span><span class="p">)</span>
<span class="n">AS</span> <span class="p">(</span><span class="n">shardid</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">shardstate</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shardlength</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">nodename</span> <span class="n">text</span><span class="p">,</span> <span class="n">nodeport</span> <span class="nb">int</span><span class="p">);</span>
</pre></div>
</div>
<p>After these commands, you can connect to any node and perform both SELECT and DML commands on the distributed table. However, DDL commands won&#8217;t be supported.</p>
</div>
</div>
<div class="section" id="append-distributed-tables">
<h2>Append Distributed Tables<a class="headerlink" href="#append-distributed-tables" title="Permalink to this headline">¶</a></h2>
<p>If your use-case does not require real-time ingests, then using append distributed tables will give you the highest ingest rates. This approach is more suitable for use-cases which use time-series data and where the database can be a few minutes or more behind.</p>
<div class="section" id="master-node-bulk-ingestion-50k-s-100k-s">
<h3>Master Node Bulk Ingestion (50k/s-100k/s)<a class="headerlink" href="#master-node-bulk-ingestion-50k-s-100k-s" title="Permalink to this headline">¶</a></h3>
<p>To ingest data into an append-distributed table, your application can first create a staging table, copy or insert data into the staging table, and finally append the staging table to the distributed table. The simplest approach is to create the staging table on the master and append the table to a new shard using master_append_table_to_shard:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Set</span> <span class="n">up</span> <span class="n">the</span> <span class="n">events</span> <span class="n">table</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">events</span> <span class="p">(</span><span class="n">time</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">data</span> <span class="n">jsonb</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;events&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;append&#39;</span><span class="p">);</span>

<span class="o">--</span> <span class="n">Add</span> <span class="n">data</span> <span class="n">into</span> <span class="n">a</span> <span class="n">new</span> <span class="n">staging</span> <span class="n">table</span>
<span class="n">CREATE</span> <span class="n">UNLOGGED</span> <span class="n">TABLE</span> <span class="n">stage_1</span> <span class="p">(</span><span class="n">LIKE</span> <span class="n">events</span><span class="p">);</span>
<span class="n">COPY</span> <span class="n">stage_1</span> <span class="n">FROM</span> <span class="s1">&#39;path-to-csv-file&#39;</span> <span class="n">WITH</span> <span class="n">CSV</span><span class="p">;</span> <span class="o">--</span> <span class="n">followed</span> <span class="n">by</span> <span class="n">CSV</span> <span class="n">data</span>

<span class="o">--</span> <span class="n">Add</span> <span class="n">the</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">staging</span> <span class="n">table</span> <span class="n">to</span> <span class="n">a</span> <span class="n">new</span> <span class="n">shard</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">events</span> <span class="n">table</span> <span class="ow">and</span> <span class="n">drop</span> <span class="n">the</span> <span class="n">staging</span> <span class="n">table</span>
<span class="n">SELECT</span> <span class="n">master_append_table_to_shard</span><span class="p">(</span><span class="n">master_create_empty_shard</span><span class="p">(</span><span class="s1">&#39;events&#39;</span><span class="p">),</span> <span class="s1">&#39;stage_1&#39;</span><span class="p">,</span> <span class="s1">&#39;master-node&#39;</span><span class="p">,</span> <span class="mi">5432</span><span class="p">);</span>
<span class="n">DROP</span> <span class="n">TABLE</span> <span class="n">stage_1</span><span class="p">;</span>
</pre></div>
</div>
<p>Note that copying to the staging table and appending to a shard need to be in separate transactions. Otherwise, the data would not yet be visible to the workers when trying to append. You can choose to make the staging table unlogged for better performance if you can easily reload the data. To learn more about the master_append_table_to_shard and master_create_empty_shard UDFs, please visit the <a class="reference internal" href="../reference/user_defined_functions.html#user-defined-functions"><span class="std std-ref">User Defined Functions Reference</span></a> section of the documentation.</p>
<p>The example above uses master_create_empty_shard to create a new shard every time new data is ingested, which allows many files to be ingested simultaneously, but may cause issues if queries end up involving thousands of shards.</p>
<p>One way to solve this problem is to define a function that selects either a new or existing shard:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">SELECT</span> <span class="n">master_append_table_to_shard</span><span class="p">(</span><span class="n">choose_shard</span><span class="p">(</span><span class="s1">&#39;events&#39;</span><span class="p">),</span> <span class="s1">&#39;stage_1&#39;</span><span class="p">,</span> <span class="s1">&#39;master-node&#39;</span><span class="p">,</span> <span class="mi">5432</span><span class="p">);</span>
</pre></div>
</div>
<p>An example of a shard selection function is given below. It appends to a shard until its size is greater than 1GB and then creates a new one, which has the drawback of only allowing one append at a time, but the advantage of bounding shard sizes.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>CREATE OR REPLACE FUNCTION choose_shard(table_id regclass) RETURNS bigint AS $$
DECLARE
  shard_id bigint;
BEGIN
  SELECT shardid INTO shard_id
  FROM pg_dist_shard JOIN pg_dist_shard_placement USING (shardid)
  WHERE logicalrelid = table_id AND shardlength &lt; 1024*1024*1024;

  IF shard_id IS NULL THEN
    /* no shard smaller than 1GB, create a new one */
    SELECT master_create_empty_shard(table_id::text) INTO shard_id;
  END IF;

  RETURN shard_id;
END;
$$ LANGUAGE plpgsql;
</pre></div>
</div>
<p>It may also be useful to create a sequence to generate a unique name for the staging table. This way each ingestion can be handled independently.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">table</span> <span class="n">name</span> <span class="n">sequence</span>
<span class="n">CREATE</span> <span class="n">SEQUENCE</span> <span class="n">stage_id_sequence</span><span class="p">;</span>

<span class="o">--</span> <span class="n">Generate</span> <span class="n">a</span> <span class="n">stage</span> <span class="n">table</span> <span class="n">name</span>
<span class="n">SELECT</span> <span class="s1">&#39;stage_&#39;</span><span class="o">||</span><span class="n">nextval</span><span class="p">(</span><span class="s1">&#39;stage_id_sequence&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="worker-node-bulk-ingestion-100k-s-1m-s">
<h3>Worker Node Bulk Ingestion (100k/s-1M/s)<a class="headerlink" href="#worker-node-bulk-ingestion-100k-s-1m-s" title="Permalink to this headline">¶</a></h3>
<p>For very high data ingestion rates, data can be staged via the workers. This method scales out horizontally and provides the highest ingestion rates, but is more complex to setup. Hence, we recommend trying this method only if your data ingestion rates cannot be addressed by the previously described methods.</p>
<p>A relatively simple way to ingest data files directly into new shards in the distributed table is to use csql, a database client that comes with Citus. csql is the same as psql, but with an additional STAGE command that copies data directly from the client to the workers into a new shard. STAGE can break up files larger than the configured citus.shard_max_size into multiple shards. The main drawbacks of STAGE are that it requires going through the command-line, only ingests files, and always creates one or more new shards.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">csql</span> <span class="o">-</span><span class="n">h</span> <span class="n">master</span><span class="o">-</span><span class="n">node</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;</span><span class="se">\\</span><span class="s2">STAGE events FROM &#39;data.csv&#39; WITH CSV&quot;</span>
</pre></div>
</div>
<p>An alternative to using STAGE is to create a staging table and use standard SQL clients to append it to the distributed table, which is similar to staging data via the master. An example of staging a file via a worker using psql is as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>stage_table=$(psql -tA -h worker-node-1 -c &quot;SELECT &#39;stage_&#39;||nextval(&#39;stage_id_sequence&#39;)&quot;)
psql -h worker-node-1 -c &quot;CREATE TABLE $stage_table (time timestamp, data jsonb)&quot;
psql -h worker-node-1 -c &quot;\\COPY $stage_table FROM &#39;data.csv&#39; WITH CSV&quot;
psql -h master-node -c &quot;SELECT master_append_table_to_shard(choose_shard(&#39;events&#39;), &#39;$stage_table&#39;, &#39;worker-node-1&#39;, 5432)&quot;
psql -h worker-node-1 -c &quot;DROP TABLE $stage_table&quot;
</pre></div>
</div>
<p>The example above again uses a choose_shard function to select the shard to which to append. To ensure parallel data ingestion, this function should balance across many different shards.</p>
<p>An example choose_shard function belows randomly picks one of the 20 smallest shards or creates a new one if there are less than 20 under 1GB. This allows 20 concurrent appends, which allows data ingestion of up to 1 million rows/s (depending on indexes, size, capacity).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>/* Choose a shard to which to append */
CREATE OR REPLACE FUNCTION choose_shard(table_id regclass)
RETURNS bigint LANGUAGE plpgsql
AS $function$
DECLARE
  shard_id bigint;
  num_small_shards int;
BEGIN
  SELECT shardid, count(*) OVER () INTO shard_id, num_small_shards
  FROM pg_dist_shard JOIN pg_dist_shard_placement USING (shardid)
  WHERE logicalrelid = table_id AND shardlength &lt; 1024*1024*1024
  GROUP BY shardid ORDER BY RANDOM() ASC;

  IF num_small_shards IS NULL OR num_small_shards &lt; 20 THEN
    SELECT master_create_empty_shard(table_id::text) INTO shard_id;
  END IF;

  RETURN shard_id;
END;
$function$;
</pre></div>
</div>
<p>A drawback of this approach is that shards may span longer time periods, which means that queries for a specific time period may involve shards that contain a lot of data outside of that period.</p>
<p>In addition to copying into temporary staging tables, it is also possible to set up tables on the workers which can continuously take INSERTs. In that case, the data has to be periodically moved into a staging table and then appended, but this requires more advanced scripting.</p>
</div>
</div>
<div class="section" id="pre-processing-data-in-citus">
<h2>Pre-processing Data in Citus<a class="headerlink" href="#pre-processing-data-in-citus" title="Permalink to this headline">¶</a></h2>
<p>The format in which raw data is delivered often differs from the schema used in the database. For example, the raw data may be in the form of log files in which every line is a JSON object, while in the database table it is more efficient to store common values in separate columns. Moreover, a distributed table should always have a distribution column. Fortunately, PostgreSQL is a very powerful data processing tool. You can apply arbitrary pre-processing using SQL before putting the results into a staging table.</p>
<p>For example, assume we have the following table schema and want to load the compressed JSON logs from <a class="reference external" href="http://www.githubarchive.org">githubarchive.org</a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">github_events</span>
<span class="p">(</span>
    <span class="n">event_id</span> <span class="n">bigint</span><span class="p">,</span>
    <span class="n">event_type</span> <span class="n">text</span><span class="p">,</span>
    <span class="n">event_public</span> <span class="n">boolean</span><span class="p">,</span>
    <span class="n">repo_id</span> <span class="n">bigint</span><span class="p">,</span>
    <span class="n">payload</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">repo</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">actor</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">org</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">created_at</span> <span class="n">timestamp</span>
<span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;github_events&#39;</span><span class="p">,</span> <span class="s1">&#39;created_at&#39;</span><span class="p">,</span> <span class="s1">&#39;append&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>To load the data, we can download the data, decompress it, filter out unsupported rows, and extract the fields in which we are interested into a staging table using 3 commands:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TEMPORARY</span> <span class="n">TABLE</span> <span class="n">prepare_1</span> <span class="p">(</span><span class="n">data</span> <span class="n">jsonb</span><span class="p">);</span>

<span class="o">/*</span> <span class="n">Load</span> <span class="n">a</span> <span class="n">file</span> <span class="n">directly</span> <span class="kn">from</span> <span class="nn">Github</span> <span class="n">archive</span> <span class="ow">and</span> <span class="nb">filter</span> <span class="n">out</span> <span class="n">rows</span> <span class="k">with</span> <span class="n">unescaped</span> <span class="mi">0</span><span class="o">-</span><span class="nb">bytes</span> <span class="o">*/</span>
<span class="n">COPY</span> <span class="n">prepare_1</span> <span class="n">FROM</span> <span class="n">PROGRAM</span>
<span class="s1">&#39;curl -s http://data.githubarchive.org/2016-01-01-15.json.gz | zcat | grep -v &quot;</span><span class="se">\\</span><span class="s1">u0000&quot;&#39;</span>
<span class="n">CSV</span> <span class="n">QUOTE</span> <span class="n">e</span><span class="s1">&#39;</span><span class="se">\x01</span><span class="s1">&#39;</span> <span class="n">DELIMITER</span> <span class="n">e</span><span class="s1">&#39;</span><span class="se">\x02</span><span class="s1">&#39;</span><span class="p">;</span>

<span class="o">/*</span> <span class="n">Prepare</span> <span class="n">a</span> <span class="n">staging</span> <span class="n">table</span> <span class="o">*/</span>
<span class="n">CREATE</span> <span class="n">UNLOGGED</span> <span class="n">TABLE</span> <span class="n">stage_1</span> <span class="n">AS</span>
<span class="n">SELECT</span> <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;id&#39;</span><span class="p">)::</span><span class="n">bigint</span> <span class="n">event_id</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="n">event_type</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;public&#39;</span><span class="p">)::</span><span class="n">boolean</span> <span class="n">event_public</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;repo&#39;</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;id&#39;</span><span class="p">)::</span><span class="n">bigint</span> <span class="n">repo_id</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;payload&#39;</span><span class="p">)</span> <span class="n">payload</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;actor&#39;</span><span class="p">)</span> <span class="n">actor</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;org&#39;</span><span class="p">)</span> <span class="n">org</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;created_at&#39;</span><span class="p">)::</span><span class="n">timestamp</span> <span class="n">created_at</span> <span class="n">FROM</span> <span class="n">prepare_1</span><span class="p">;</span>
</pre></div>
</div>
<p>You can then use the master_append_table_to_shard function to append this staging table to the distributed table.</p>
<p>This approach works especially well when staging data via the workers, since the pre-processing itself can be scaled out by running it on many workers in parallel for different chunks of input data.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="performance_tuning.html" class="btn btn-neutral float-right" title="Query Performance Tuning" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="query_processing.html" class="btn btn-neutral" title="Citus Query Processing" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Citus Data.

    </p>
  </div> 

</footer>

          <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-32858865-1', 'auto');
  ga('send', 'pageview');

</script>
<script>
/**
* Function that tracks a click on an outbound link in Analytics.
* This function takes a valid URL string as an argument, and uses that URL string
* as the event label. Setting the transport method to 'beacon' lets the hit be sent
* using 'navigator.sendBeacon' in browser that support it.
*/
var trackOutboundLink = function(url) {
   ga('send', 'event', 'outbound', 'click', url, {
     'transport': 'beacon',
     'hitCallback': function(){document.location = url;}
   });
}
</script>

        </div>
      </div>

    </section>

  </div>
  
<h3>Versions</h3>
<ul>
    <li><a href="../../v5.2/performance/scaling_data_ingestion.html">v5.2</a></li>
    <li><a href="../../v5.1/performance/scaling_data_ingestion.html">v5.1</a></li>
    <li><a href="scaling_data_ingestion.html">v5.0</a></li>
</ul>

  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'5.0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>