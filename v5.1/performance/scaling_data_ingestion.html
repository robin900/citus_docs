

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scaling Out Data Ingestion &mdash; Citus 5.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="Citus 5.1.0 documentation" href="../index.html"/>
        <link rel="next" title="Query Performance Tuning" href="performance_tuning.html"/>
        <link rel="prev" title="Citus Query Processing" href="query_processing.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">
  <div class="header">
  <div class="section-wrap"> 
              <div class="utility-nav">
              </div>
                      <a href="https://www.citusdata.com" title="Citus Data" rel="home" class="logo">
            <img src="../_static/images/citus-logo.png" alt="Citus Data">
          </a>
              <div class="clear"></div>
  </div>
  </div>
  <div class="nav-wrap menu">
    <div class="section-wrap">
      <ul>
        <li><a href="https://www.citusdata.com/product/citus">Product</a></li>
        <li><a href="https://www.citusdata.com/solutions/applications">Solutions</a></li>
        <li><a href="https://www.citusdata.com/blog">Blog</a></li>
        <li><a href="#">Documentation</a></li>
      </ul>
    </div>
  </div>

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Citus
          

          
          </a>

          
            
            
              <div class="version">
                5.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">About Citus</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../aboutcitus/what_is_citus.html">What is Citus?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aboutcitus/introduction_to_citus.html">Architecture</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-cluster.html">Start Demo Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-hash-distribution.html">Hash-Distributed Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tut-append-distribution.html">Append-Distributed Data</a></li>
</ul>
<p class="caption"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/requirements.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/development.html">Single-Machine Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/production.html">Multi-Machine Cluster</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Tables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/working_with_distributed_tables.html">Working with distributed tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/hash_distribution.html">Hash Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/append_distribution.html">Append Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/querying.html">Querying Distributed Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_tables/postgresql_extensions.html">PostgreSQL extensions</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="query_processing.html">Citus Query Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Scaling Out Data Ingestion</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hash-distributed-tables">Hash Distributed Tables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#real-time-inserts-0-50k-s">Real-time Inserts (0-50k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#real-time-updates-0-50k-s">Real-time Updates (0-50k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bulk-copy-100-200k-s">Bulk Copy (100-200k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#masterless-citus-50k-s-500k-s">Masterless Citus (50k/s-500k/s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#append-distributed-tables">Append Distributed Tables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#master-node-bulk-ingestion-100k-s-200k-s">Master Node Bulk Ingestion (100k/s-200k/s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#worker-node-bulk-ingestion-100k-s-1m-s">Worker Node Bulk Ingestion (100k/s-1M/s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pre-processing-data-in-citus">Pre-processing Data in Citus</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Query Performance Tuning</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cloud/index.html">Citus Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud/features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud/support.html">Support and Billing</a></li>
</ul>
<p class="caption"><span class="caption-text">Administration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/cluster_management.html">Cluster Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/upgrading_citus.html">Upgrading Citus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin_guide/transitioning_from_postgresql_to_citus.html">Transitioning From PostgreSQL to Citus</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/citus_sql_reference.html">Citus SQL Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/user_defined_functions.html">User Defined Functions Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/metadata_tables.html">Metadata Tables Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/configuration.html">Configuration Reference</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/faq.html">Frequently Asked Questions</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <a href="../index.html">Citus</a>
      <ul>
        <li><a href="/product/citus">Product</a></li>
        <li><a href="/solutions/applications">Solutions</a></li>
        <li><a href="/blog">Blog</a></li>
      </ul>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li>Scaling Out Data Ingestion</li>
    <li class="wy-breadcrumbs-aside">
      
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scaling-out-data-ingestion">
<span id="scaling-data-ingestion"></span><h1>Scaling Out Data Ingestion<a class="headerlink" href="#scaling-out-data-ingestion" title="Permalink to this headline">¶</a></h1>
<p>Citus lets you scale out data ingestion to very high rates, but there are several trade-offs to consider in terms of the throughput, durability, consistency and latency. In this section, we discuss several approaches to data ingestion and give examples of how to use them.</p>
<p>The best method to distribute tables and ingest your data depends on your use case requirements. Citus supports two distribution methods: append and hash; and the data ingestion methods differ between them. You can visit the <a class="reference internal" href="../dist_tables/working_with_distributed_tables.html#working-with-distributed-tables"><span class="std std-ref">Working with distributed tables</span></a> section to learn about the tradeoffs associated with each distribution method.</p>
<div class="section" id="hash-distributed-tables">
<h2>Hash Distributed Tables<a class="headerlink" href="#hash-distributed-tables" title="Permalink to this headline">¶</a></h2>
<p>Hash distributed tables support ingestion using standard single row INSERT and UPDATE commands, as well as bulk ingestion through COPY.</p>
<div class="section" id="real-time-inserts-0-50k-s">
<h3>Real-time Inserts (0-50k/s)<a class="headerlink" href="#real-time-inserts-0-50k-s" title="Permalink to this headline">¶</a></h3>
<p>On the Citus master, you can perform INSERT commands directly on hash distributed tables. The advantage of using INSERT is that the new data is immediately visible to SELECT queries, and durably stored on multiple replicas.</p>
<p>When processing an INSERT, Citus first finds the right shard placements based on the value in the distribution column, then it connects to the workers storing the shard placements, and finally performs an INSERT on each of them. From the perspective of the user, the INSERT takes several milliseconds to process because of the round-trips to the workers, but the master can process other INSERTs in other sessions while waiting for a response. The master also keeps connections to the workers open within the same session, which means subsequent queries will see lower response times.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Set</span> <span class="n">up</span> <span class="n">a</span> <span class="n">distributed</span> <span class="n">table</span> <span class="n">containing</span> <span class="n">counters</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">counters</span> <span class="p">(</span><span class="n">c_key</span> <span class="n">text</span><span class="p">,</span> <span class="n">c_date</span> <span class="n">date</span><span class="p">,</span> <span class="n">c_value</span> <span class="nb">int</span><span class="p">,</span> <span class="n">primary</span> <span class="n">key</span> <span class="p">(</span><span class="n">c_key</span><span class="p">,</span> <span class="n">c_date</span><span class="p">));</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;counters&#39;</span><span class="p">,</span> <span class="s1">&#39;c_key&#39;</span><span class="p">,</span> <span class="s1">&#39;hash&#39;</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_worker_shards</span><span class="p">(</span><span class="s1">&#39;counters&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>

<span class="o">--</span> <span class="n">Enable</span> <span class="n">timing</span> <span class="n">to</span> <span class="n">see</span> <span class="n">reponse</span> <span class="n">times</span>
\<span class="n">timing</span> <span class="n">on</span>

<span class="o">--</span> <span class="n">First</span> <span class="n">INSERT</span> <span class="n">requires</span> <span class="n">connection</span> <span class="nb">set</span><span class="o">-</span><span class="n">up</span><span class="p">,</span> <span class="n">second</span> <span class="n">will</span> <span class="n">be</span> <span class="n">faster</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-04&#39;</span><span class="p">,</span> <span class="mi">12</span><span class="p">);</span> <span class="o">--</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">10.314</span> <span class="n">ms</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-05&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span> <span class="o">--</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">3.132</span> <span class="n">ms</span>
</pre></div>
</div>
<p>To reach high throughput rates, applications should send INSERTs over a many separate connections and keep connections open to avoid the initial overhead of connection set-up.</p>
</div>
<div class="section" id="real-time-updates-0-50k-s">
<h3>Real-time Updates (0-50k/s)<a class="headerlink" href="#real-time-updates-0-50k-s" title="Permalink to this headline">¶</a></h3>
<p>On the Citus master, you can also perform UPDATE, DELETE, and INSERT ... ON CONFLICT (UPSERT) commands on distributed tables. By default, these queries take an exclusive lock on the shard, which prevents concurrent modifications to guarantee that the commands are applied in the same order on all shard placements.</p>
<p>Given that every command requires several round-trips to the workers, and no two commands can run on the same shard at the same time, update throughput is very low by default. However, if you know that the order of the queries doesn&#8217;t matter (they are commutative), then you can turn on citus.all_modifications_commutative, in which case multiple commands can update the same shard concurrently.</p>
<p>For example, if your distributed table contains counters and all your DML queries are UPSERTs that add to the counters, then you can safely turn on citus.all_modifications_commutative since addition is commutative:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">SET</span> <span class="n">citus</span><span class="o">.</span><span class="n">all_modifications_commutative</span> <span class="n">TO</span> <span class="n">on</span><span class="p">;</span>
<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">counters</span> <span class="n">VALUES</span> <span class="p">(</span><span class="s1">&#39;num_purchases&#39;</span><span class="p">,</span> <span class="s1">&#39;2016-03-04&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ON</span> <span class="n">CONFLICT</span> <span class="p">(</span><span class="n">c_key</span><span class="p">,</span> <span class="n">c_date</span><span class="p">)</span> <span class="n">DO</span> <span class="n">UPDATE</span> <span class="n">SET</span> <span class="n">c_value</span> <span class="o">=</span> <span class="n">counters</span><span class="o">.</span><span class="n">c_value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</pre></div>
</div>
<p>Note that this query also takes an exclusive lock on the row in PostgreSQL, which may also limit the throughput. When storing counters, consider that using INSERT and summing values in a SELECT does not require exclusive locks.</p>
<p>When the replication factor is 1, it is always safe to enable citus.all_modifications_commutative. Citus does not do this automatically yet.</p>
</div>
<div class="section" id="bulk-copy-100-200k-s">
<h3>Bulk Copy (100-200k/s)<a class="headerlink" href="#bulk-copy-100-200k-s" title="Permalink to this headline">¶</a></h3>
<p>Hash distributed tables support <a class="reference external" href="http://www.postgresql.org/docs/current/static/sql-copy.html">COPY</a> from the Citus master for bulk ingestion, which can achieve much higher ingestion rates than regular INSERT statements.</p>
<p>COPY can be used to load data directly from an application using COPY .. FROM STDIN, or from a file on the server or program executed on the server.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">COPY</span> <span class="n">counters</span> <span class="n">FROM</span> <span class="n">STDIN</span> <span class="n">WTIH</span> <span class="p">(</span><span class="n">FORMAT</span> <span class="n">CSV</span><span class="p">);</span>
</pre></div>
</div>
<p>In psql, the \COPY command can be used to load data from the local machine. The \COPY command actually sends a COPY .. FROM STDIN command to the server before sending the local data, as would an application that loads data directly.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">psql</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;\COPY counters FROM &#39;counters-20160304.csv&#39; (FORMAT CSV)&quot;</span>
</pre></div>
</div>
<p>A very powerful feature of COPY for hash distributed tables is that it asynchronously copies data to the workers over many parallel connections, one for each shard placement. This means that data can be ingested using multiple workers and multiple cores in parallel. Especially when there are expensive indexes such as a GIN, this can lead to major performance boosts over ingesting into a regular PostgreSQL table.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To avoid opening too many connections to the workers. We recommend only running only one COPY command on a hash distributed table at a time. In practice, running more than two at a time rarely results in performance benefits. An exception is when all the data in the ingested file has a specific partition key value, which goes into a single shard. COPY will only open connections to shards when necessary.</p>
</div>
</div>
<div class="section" id="masterless-citus-50k-s-500k-s">
<h3>Masterless Citus (50k/s-500k/s)<a class="headerlink" href="#masterless-citus-50k-s-500k-s" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This section is currently experimental and not a guide to setup masterless clusters in production. We are working on providing official support for masterless clusters including replication and automated fail-over solutions. Please contact us at <a class="reference external" href="mailto:engage&#37;&#52;&#48;citusdata&#46;com">engage<span>&#64;</span>citusdata<span>&#46;</span>com</a> if your use-case requires multiple masters.</p>
</div>
<p>It is technically possible to create the distributed table on every node in the cluster. The big advantage is that all  queries on distributed tables can be performed at a very high rate by spreading the queries across the workers. In this case, the replication factor should always be 1 to ensure consistency, which causes data to become unavailable when a node goes down. All nodes should have a hot standby and automated fail-over to ensure high availability.</p>
<p>To allow DML commands on the distribute table from any node, first create a distributed table on both the master and the workers:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">data</span> <span class="p">(</span><span class="n">key</span> <span class="n">text</span><span class="p">,</span> <span class="n">value</span> <span class="n">text</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;key&#39;</span><span class="p">,</span><span class="s1">&#39;hash&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>Then on the master, create shards for the distributed table with a replication factor of 1.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="mi">128</span> <span class="n">shards</span> <span class="k">with</span> <span class="n">a</span> <span class="n">single</span> <span class="n">replica</span> <span class="n">on</span> <span class="n">the</span> <span class="n">workers</span>
<span class="n">SELECT</span> <span class="n">master_create_worker_shards</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
<p>Finally, you need to copy and convert the shard metadata from the master to the workers. The logicalrelid column in pg_dist_shard may differ per node. If you have the dblink extension installed, then you can run the following commands on the workers to get the metadata from master-node.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">pg_dist_shard</span> <span class="n">SELECT</span> <span class="o">*</span> <span class="n">FROM</span>
<span class="n">dblink</span><span class="p">(</span><span class="s1">&#39;host=master-node port=5432&#39;</span><span class="p">,</span>
       <span class="s1">&#39;SELECT logicalrelid::regclass,shardid,shardstorage,shardalias,shardminvalue,shardmaxvalue FROM pg_dist_shard&#39;</span><span class="p">)</span>
<span class="n">AS</span> <span class="p">(</span><span class="n">logicalrelid</span> <span class="n">regclass</span><span class="p">,</span> <span class="n">shardid</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">shardstorage</span> <span class="n">char</span><span class="p">,</span> <span class="n">shardalias</span> <span class="n">text</span><span class="p">,</span> <span class="n">shardminvalue</span> <span class="n">text</span><span class="p">,</span> <span class="n">shardmaxvalue</span> <span class="n">text</span><span class="p">);</span>

<span class="n">INSERT</span> <span class="n">INTO</span> <span class="n">pg_dist_shard_placement</span> <span class="n">SELECT</span> <span class="o">*</span> <span class="n">FROM</span>
<span class="n">dblink</span><span class="p">(</span><span class="s1">&#39;host=master-node port=5432&#39;</span><span class="p">,</span>
       <span class="s1">&#39;SELECT * FROM pg_dist_shard_placement&#39;</span><span class="p">)</span>
<span class="n">AS</span> <span class="p">(</span><span class="n">shardid</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">shardstate</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shardlength</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">nodename</span> <span class="n">text</span><span class="p">,</span> <span class="n">nodeport</span> <span class="nb">int</span><span class="p">);</span>
</pre></div>
</div>
<p>After these commands, you can connect to any node and perform both SELECT and DML commands on the distributed table. However, DDL commands won&#8217;t be supported.</p>
</div>
</div>
<div class="section" id="append-distributed-tables">
<h2>Append Distributed Tables<a class="headerlink" href="#append-distributed-tables" title="Permalink to this headline">¶</a></h2>
<p>If your use-case does not require real-time ingests, then using append distributed tables will give you the highest ingest rates. This approach is more suitable for use-cases which use time-series data and where the database can be a few minutes or more behind.</p>
<div class="section" id="master-node-bulk-ingestion-100k-s-200k-s">
<h3>Master Node Bulk Ingestion (100k/s-200k/s)<a class="headerlink" href="#master-node-bulk-ingestion-100k-s-200k-s" title="Permalink to this headline">¶</a></h3>
<p>To ingest data into an append distributed table, you can use the <a class="reference external" href="http://www.postgresql.org/docs/current/static/sql-copy.html">COPY</a> command, which will create a new shard out of the data you ingest. COPY can break up files larger than the configured citus.shard_max_size into multiple shards. COPY for append distributed tables only opens connections for the new shards, which means it behaves a bit differently than COPY for hash distributed tables, which may open connections for all shards. A COPY for append distributed tables command does not ingest rows in parallel over many connections, but it is safe to run many commands in parallel.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Set</span> <span class="n">up</span> <span class="n">the</span> <span class="n">events</span> <span class="n">table</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">events</span> <span class="p">(</span><span class="n">time</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">data</span> <span class="n">jsonb</span><span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;events&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;append&#39;</span><span class="p">);</span>

<span class="o">--</span> <span class="n">Add</span> <span class="n">data</span> <span class="n">into</span> <span class="n">a</span> <span class="n">new</span> <span class="n">staging</span> <span class="n">table</span>
\<span class="n">COPY</span> <span class="n">events</span> <span class="n">FROM</span> <span class="s1">&#39;path-to-csv-file&#39;</span> <span class="n">WITH</span> <span class="n">CSV</span>
</pre></div>
</div>
<p>COPY creates new shards every time it is used, which allows many files to be ingested simultaneously, but may cause issues if queries end up involving thousands of shards. An alternative way to ingest data is to append it to existing shards using the master_append_table_to_shard function. To use master_append_table_to_shard, the data needs to be loaded into a staging table and some custom logic to select an appropriate shard is required.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Prepare</span> <span class="n">a</span> <span class="n">staging</span> <span class="n">table</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">stage_1</span> <span class="p">(</span><span class="n">LIKE</span> <span class="n">events</span><span class="p">);</span>
\<span class="n">COPY</span> <span class="n">stage_1</span> <span class="n">FROM</span> <span class="s1">&#39;path-to-csv-file WITH CSV</span>

<span class="o">--</span> <span class="n">In</span> <span class="n">a</span> <span class="n">separate</span> <span class="n">transaction</span><span class="p">,</span> <span class="n">append</span> <span class="n">the</span> <span class="n">staging</span> <span class="n">table</span>
<span class="n">SELECT</span> <span class="n">master_append_table_to_shard</span><span class="p">(</span><span class="n">select_events_shard</span><span class="p">(),</span> <span class="s1">&#39;stage_1&#39;</span><span class="p">,</span> <span class="s1">&#39;master-node&#39;</span><span class="p">,</span> <span class="mi">5432</span><span class="p">);</span>
</pre></div>
</div>
<p>An example of a shard selection function is given below. It appends to a shard until its size is greater than 1GB and then creates a new one, which has the drawback of only allowing one append at a time, but the advantage of bounding shard sizes.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>CREATE OR REPLACE FUNCTION select_events_shard() RETURNS bigint AS $$
DECLARE
  shard_id bigint;
BEGIN
  SELECT shardid INTO shard_id
  FROM pg_dist_shard JOIN pg_dist_shard_placement USING (shardid)
  WHERE logicalrelid = &#39;events&#39;::regclass AND shardlength &lt; 1024*1024*1024;

  IF shard_id IS NULL THEN
    /* no shard smaller than 1GB, create a new one */
    SELECT master_create_empty_shard(&#39;events&#39;) INTO shard_id;
  END IF;

  RETURN shard_id;
END;
$$ LANGUAGE plpgsql;
</pre></div>
</div>
<p>It may also be useful to create a sequence to generate a unique name for the staging table. This way each ingestion can be handled independently.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">table</span> <span class="n">name</span> <span class="n">sequence</span>
<span class="n">CREATE</span> <span class="n">SEQUENCE</span> <span class="n">stage_id_sequence</span><span class="p">;</span>

<span class="o">--</span> <span class="n">Generate</span> <span class="n">a</span> <span class="n">stage</span> <span class="n">table</span> <span class="n">name</span>
<span class="n">SELECT</span> <span class="s1">&#39;stage_&#39;</span><span class="o">||</span><span class="n">nextval</span><span class="p">(</span><span class="s1">&#39;stage_id_sequence&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>To learn more about the master_append_table_to_shard and master_create_empty_shard UDFs, please visit the <a class="reference internal" href="../reference/user_defined_functions.html#user-defined-functions"><span class="std std-ref">User Defined Functions Reference</span></a> section of the documentation.</p>
</div>
<div class="section" id="worker-node-bulk-ingestion-100k-s-1m-s">
<h3>Worker Node Bulk Ingestion (100k/s-1M/s)<a class="headerlink" href="#worker-node-bulk-ingestion-100k-s-1m-s" title="Permalink to this headline">¶</a></h3>
<p>For very high data ingestion rates, data can be staged via the workers. This method scales out horizontally and provides the highest ingestion rates, but can be more complex to use. Hence, we recommend trying this method only if your data ingestion rates cannot be addressed by the previously described methods.</p>
<p>Append distributed tables support COPY via the worker, by specifying the address of the master in a master_host option, and optionally a master_port option (defaults to 5432). COPY via the workers has the same general properties as COPY via the master, except the initial parsing is not bottlenecked on the master.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">psql</span> <span class="o">-</span><span class="n">h</span> <span class="n">worker</span><span class="o">-</span><span class="n">node</span><span class="o">-</span><span class="mi">1</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;\COPY events FROM &#39;data.csv&#39; WITH (FORMAT CSV, MASTER_HOST &#39;master-node&#39;)&quot;</span>
</pre></div>
</div>
<p>An alternative to using COPY is to create a staging table and use standard SQL clients to append it to the distributed table, which is similar to staging data via the master. An example of staging a file via a worker using psql is as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>stage_table=$(psql -tA -h worker-node-1 -c &quot;SELECT &#39;stage_&#39;||nextval(&#39;stage_id_sequence&#39;)&quot;)
psql -h worker-node-1 -c &quot;CREATE TABLE $stage_table (time timestamp, data jsonb)&quot;
psql -h worker-node-1 -c &quot;\COPY $stage_table FROM &#39;data.csv&#39; WITH CSV&quot;
psql -h master-node -c &quot;SELECT master_append_table_to_shard(choose_underutilized_shard(), &#39;$stage_table&#39;, &#39;worker-node-1&#39;, 5432)&quot;
psql -h worker-node-1 -c &quot;DROP TABLE $stage_table&quot;
</pre></div>
</div>
<p>The example above uses a choose_underutilized_shard function to select the shard to which to append. To ensure parallel data ingestion, this function should balance across many different shards.</p>
<p>An example choose_underutilized_shard function belows randomly picks one of the 20 smallest shards or creates a new one if there are less than 20 under 1GB. This allows 20 concurrent appends, which allows data ingestion of up to 1 million rows/s (depending on indexes, size, capacity).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>/* Choose a shard to which to append */
CREATE OR REPLACE FUNCTION choose_underutilized_shard()
RETURNS bigint LANGUAGE plpgsql
AS $function$
DECLARE
  shard_id bigint;
  num_small_shards int;
BEGIN
  SELECT shardid, count(*) OVER () INTO shard_id, num_small_shards
  FROM pg_dist_shard JOIN pg_dist_shard_placement USING (shardid)
  WHERE logicalrelid = &#39;events&#39;::regclass AND shardlength &lt; 1024*1024*1024
  GROUP BY shardid ORDER BY RANDOM() ASC;

  IF num_small_shards IS NULL OR num_small_shards &lt; 20 THEN
    SELECT master_create_empty_shard(&#39;events&#39;) INTO shard_id;
  END IF;

  RETURN shard_id;
END;
$function$;
</pre></div>
</div>
<p>A drawback of ingesting into many shards concurrently is that shards may span longer time ranges, which means that queries for a specific time period may involve shards that contain a lot of data outside of that period.</p>
<p>In addition to copying into temporary staging tables, it is also possible to set up tables on the workers which can continuously take INSERTs. In that case, the data has to be periodically moved into a staging table and then appended, but this requires more advanced scripting.</p>
</div>
</div>
<div class="section" id="pre-processing-data-in-citus">
<h2>Pre-processing Data in Citus<a class="headerlink" href="#pre-processing-data-in-citus" title="Permalink to this headline">¶</a></h2>
<p>The format in which raw data is delivered often differs from the schema used in the database. For example, the raw data may be in the form of log files in which every line is a JSON object, while in the database table it is more efficient to store common values in separate columns. Moreover, a distributed table should always have a distribution column. Fortunately, PostgreSQL is a very powerful data processing tool. You can apply arbitrary pre-processing using SQL before putting the results into a staging table.</p>
<p>For example, assume we have the following table schema and want to load the compressed JSON logs from <a class="reference external" href="http://www.githubarchive.org">githubarchive.org</a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">github_events</span>
<span class="p">(</span>
    <span class="n">event_id</span> <span class="n">bigint</span><span class="p">,</span>
    <span class="n">event_type</span> <span class="n">text</span><span class="p">,</span>
    <span class="n">event_public</span> <span class="n">boolean</span><span class="p">,</span>
    <span class="n">repo_id</span> <span class="n">bigint</span><span class="p">,</span>
    <span class="n">payload</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">repo</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">actor</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">org</span> <span class="n">jsonb</span><span class="p">,</span>
    <span class="n">created_at</span> <span class="n">timestamp</span>
<span class="p">);</span>
<span class="n">SELECT</span> <span class="n">master_create_distributed_table</span><span class="p">(</span><span class="s1">&#39;github_events&#39;</span><span class="p">,</span> <span class="s1">&#39;created_at&#39;</span><span class="p">,</span> <span class="s1">&#39;append&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>To load the data, we can download the data, decompress it, filter out unsupported rows, and extract the fields in which we are interested into a staging table using 3 commands:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TEMPORARY</span> <span class="n">TABLE</span> <span class="n">prepare_1</span> <span class="p">(</span><span class="n">data</span> <span class="n">jsonb</span><span class="p">);</span>

<span class="o">--</span> <span class="n">Load</span> <span class="n">a</span> <span class="n">file</span> <span class="n">directly</span> <span class="kn">from</span> <span class="nn">Github</span> <span class="n">archive</span> <span class="ow">and</span> <span class="nb">filter</span> <span class="n">out</span> <span class="n">rows</span> <span class="k">with</span> <span class="n">unescaped</span> <span class="mi">0</span><span class="o">-</span><span class="nb">bytes</span>
<span class="n">COPY</span> <span class="n">prepare_1</span> <span class="n">FROM</span> <span class="n">PROGRAM</span>
<span class="s1">&#39;curl -s http://data.githubarchive.org/2016-01-01-15.json.gz | zcat | grep -v &quot;</span><span class="se">\\</span><span class="s1">u0000&quot;&#39;</span>
<span class="n">CSV</span> <span class="n">QUOTE</span> <span class="n">e</span><span class="s1">&#39;</span><span class="se">\x01</span><span class="s1">&#39;</span> <span class="n">DELIMITER</span> <span class="n">e</span><span class="s1">&#39;</span><span class="se">\x02</span><span class="s1">&#39;</span><span class="p">;</span>

<span class="o">--</span> <span class="n">Prepare</span> <span class="n">a</span> <span class="n">staging</span> <span class="n">table</span>
<span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">stage_1</span> <span class="n">AS</span>
<span class="n">SELECT</span> <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;id&#39;</span><span class="p">)::</span><span class="n">bigint</span> <span class="n">event_id</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="n">event_type</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;public&#39;</span><span class="p">)::</span><span class="n">boolean</span> <span class="n">event_public</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;repo&#39;</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;id&#39;</span><span class="p">)::</span><span class="n">bigint</span> <span class="n">repo_id</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;payload&#39;</span><span class="p">)</span> <span class="n">payload</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;actor&#39;</span><span class="p">)</span> <span class="n">actor</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="s1">&#39;org&#39;</span><span class="p">)</span> <span class="n">org</span><span class="p">,</span>
       <span class="p">(</span><span class="n">data</span><span class="o">-&gt;&gt;</span><span class="s1">&#39;created_at&#39;</span><span class="p">)::</span><span class="n">timestamp</span> <span class="n">created_at</span> <span class="n">FROM</span> <span class="n">prepare_1</span><span class="p">;</span>
</pre></div>
</div>
<p>You can then use the master_append_table_to_shard function to append this staging table to the distributed table.</p>
<p>This approach works especially well when staging data via the workers, since the pre-processing itself can be scaled out by running it on many workers in parallel for different chunks of input data.</p>
<p>For a more complete example, see <a class="reference external" href="https://www.citusdata.com/blog/14-marco/402-interactive-analytics-github-data-using-postgresql-citus">Interactive Analytics on GitHub Data using PostgreSQL with Citus</a>.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="performance_tuning.html" class="btn btn-neutral float-right" title="Query Performance Tuning" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="query_processing.html" class="btn btn-neutral" title="Citus Query Processing" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Citus Data.

    </p>
  </div> 

</footer>

          <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-32858865-1', 'auto');
  ga('send', 'pageview');

</script>
<script>
/**
* Function that tracks a click on an outbound link in Analytics.
* This function takes a valid URL string as an argument, and uses that URL string
* as the event label. Setting the transport method to 'beacon' lets the hit be sent
* using 'navigator.sendBeacon' in browser that support it.
*/
var trackOutboundLink = function(url) {
   ga('send', 'event', 'outbound', 'click', url, {
     'transport': 'beacon',
     'hitCallback': function(){document.location = url;}
   });
}
</script>

        </div>
      </div>

    </section>

  </div>
  
<h3>Versions</h3>
<ul>
    <li><a href="../../v5.2/performance/scaling_data_ingestion.html">v5.2</a></li>
    <li><a href="scaling_data_ingestion.html">v5.1</a></li>
    <li><a href="../../v5.0/performance/scaling_data_ingestion.html">v5.0</a></li>
</ul>

  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'5.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>